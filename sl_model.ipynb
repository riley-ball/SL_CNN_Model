{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "\n",
    "# Specify the name of the dataset and the path where you want to save it\n",
    "dataset_name = \"datamunge/sign-language-mnist\"\n",
    "save_path = \"./sl_data\"\n",
    "\n",
    "# Download the dataset\n",
    "kaggle.api.dataset_download_files(dataset_name, path=save_path, unzip=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into Dataframes & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"./sl_data/sign_mnist_train.csv\")\n",
    "df_test = pd.read_csv(\"./sl_data/sign_mnist_test.csv\")\n",
    "\n",
    "# Print the column names and types\n",
    "column_types = df.dtypes\n",
    "columns_names = df.columns\n",
    "# Get column_names and column_types as lists\n",
    "column_name = list(columns_names)\n",
    "column_type = list(column_types)\n",
    "\n",
    "# print number of unique values in the label column\n",
    "unique_labels = df['label'].max()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 785)\n",
      "(7172, 785)\n"
     ]
    }
   ],
   "source": [
    "# print number of rows for df and df_test\n",
    "print(df.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise Train and Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise train and test data\n",
    "\n",
    "# get the pixel columns\n",
    "pixel_columns = df.columns[1:]\n",
    "# normalize the pixel columns\n",
    "df[pixel_columns] = df[pixel_columns] / 255\n",
    "df_test[pixel_columns] = df_test[pixel_columns] / 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load into Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe to tensorflow dataframe that returns (image, label) instead of a dictionary and reshape to (32, 28, 28, 1)\n",
    "import tensorflow as tf\n",
    "train = tf.data.Dataset.from_tensor_slices((df[pixel_columns].values, df['label'].values))\n",
    "test = tf.data.Dataset.from_tensor_slices((df_test[pixel_columns].values, df_test['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-addons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Define the CNN model\n",
    "num_filters = 64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape((28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(num_filters, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(unique_labels+1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "858/858 [==============================] - 8s 8ms/step - loss: 2.6095 - accuracy: 0.2859\n",
      "Epoch 2/5\n",
      "858/858 [==============================] - 7s 8ms/step - loss: 1.4666 - accuracy: 0.6134\n",
      "Epoch 3/5\n",
      "858/858 [==============================] - 7s 8ms/step - loss: 0.9920 - accuracy: 0.7378\n",
      "Epoch 4/5\n",
      "858/858 [==============================] - 7s 8ms/step - loss: 0.7391 - accuracy: 0.8087\n",
      "Epoch 5/5\n",
      "858/858 [==============================] - 7s 8ms/step - loss: 0.5737 - accuracy: 0.8575\n",
      "858/858 [==============================] - 2s 2ms/step - loss: 0.4896 - accuracy: 0.8894\n",
      "Train Accuracy:  0.8893826007843018\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.8610 - accuracy: 0.7394\n",
      "Test accuracy: 0.739403247833252\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "train = train.batch(32) # Convert to a BatchDataset\n",
    "model.fit(train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(train)\n",
    "print('Train Accuracy: ', accuracy)\n",
    "\n",
    "test = test.batch(32)\n",
    "test_loss, test_acc = model.evaluate(test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, 'model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress Model into .zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile('sign_language_model.zip', 'w') as zip_obj:\n",
    "    for folderName, subfolders, filenames in os.walk(\"model\"):\n",
    "        for filename in filenames:\n",
    "            filePath = os.path.join(folderName, filename)\n",
    "            zip_obj.write(filePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5eaf1c2e499f88d5fc65844dfc502993f27721fe13a5a8555989bff728548410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
